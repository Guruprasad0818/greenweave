"""
GreenWeave SDK â€” Drop-in OpenAI replacement
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Change ONE line. Your entire app becomes carbon-aware.

BEFORE (standard OpenAI):
    from openai import OpenAI
    client = OpenAI(api_key="sk-...")

AFTER (GreenWeave â€” zero other changes needed):
    from greenweave_sdk import GreenWeave as OpenAI
    client = OpenAI(api_key="sk-...", greenweave_url="http://localhost:8000")

Every .chat.completions.create() call is now:
  âœ… Carbon-routed to the optimal model
  âœ… Semantically cached (repeat queries = 0 COâ‚‚)
  âœ… Logged to ESG database
  âœ… Budget-tracked
  âœ… Returns OpenAI-compatible response object (zero breaking changes)

Install:
    pip install greenweave-sdk
    # or locally:
    pip install -e ./greenweave_sdk
"""

import time
import uuid
import requests
from typing import Iterator

from greenweave_sdk.models import (
    ChatCompletion,
    ChatCompletionChunk,
    Choice,
    Message,
    Usage,
    CarbonReceipt,
)


class _Completions:
    """
    Mimics openai.resources.chat.Completions
    Intercepts .create() and routes through GreenWeave.
    """

    def __init__(self, client: "GreenWeave"):
        self._client = client

    def create(
        self,
        model: str = "gpt-4",           # accepted but overridden by carbon router
        messages: list[dict] = None,
        system_prompt: str = None,
        task_type: str = "casual_chat",
        weight_profile: str = "BALANCED",
        skip_cache: bool = False,
        stream: bool = False,           # stream=True returns fake streamed chunks
        max_tokens: int = 1024,
        temperature: float = 0.7,
        **kwargs,                       # absorb any other OpenAI params silently
    ) -> "ChatCompletion":
        """
        Drop-in replacement for openai.chat.completions.create()

        Extra GreenWeave params (all optional):
            task_type     : "casual_chat" | "summarization" | "coding" |
                            "legal_drafting" | "medical"
            weight_profile: "BALANCED" | "ECO_FIRST" | "ACCURACY_FIRST"
            skip_cache    : Force LLM call even if cache hit exists
        """
        if messages is None:
            messages = []

        t_start = time.perf_counter()

        payload = {
            "messages":       messages,
            "task_type":      task_type,
            "weight_profile": weight_profile,
            "skip_cache":     skip_cache,
        }
        if system_prompt:
            payload["system_prompt"] = system_prompt

        try:
            resp = requests.post(
                f"{self._client.greenweave_url}/chat/completions",
                json=payload,
                timeout=self._client.timeout,
                headers={"X-GreenWeave-Team": self._client.team or "default"},
            )
            resp.raise_for_status()
            data = resp.json()
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                f"GreenWeave router unreachable at {self._client.greenweave_url}. "
                "Is the router running? Check: docker-compose up"
            )
        except requests.exceptions.Timeout:
            raise TimeoutError(
                f"GreenWeave router timed out after {self._client.timeout}s."
            )
        except requests.exceptions.HTTPError as e:
            raise RuntimeError(f"GreenWeave router returned error: {e}")

        total_ms = round((time.perf_counter() - t_start) * 1000, 1)

        response_text = data.get("response", "")
        receipt_raw   = data.get("carbon_receipt", {})
        model_used    = data.get("model_used", "unknown")

        # Build OpenAI-compatible response object
        completion = ChatCompletion(
            id=f"gw-{uuid.uuid4().hex[:12]}",
            object="chat.completion",
            created=int(time.time()),
            model=model_used,
            choices=[
                Choice(
                    index=0,
                    message=Message(role="assistant", content=response_text),
                    finish_reason="stop",
                )
            ],
            usage=Usage(
                prompt_tokens=len(str(messages)) // 4,   # rough estimate
                completion_tokens=len(response_text) // 4,
                total_tokens=(len(str(messages)) + len(response_text)) // 4,
            ),
            # GreenWeave extension â€” not in standard OpenAI but additive
            carbon_receipt=CarbonReceipt(**{
                "mode":                    receipt_raw.get("mode", "UNKNOWN"),
                "model_used":              model_used,
                "grid_intensity_gco2_kwh": receipt_raw.get("grid_intensity_gco2_kwh", 0),
                "co2_this_query_g":        receipt_raw.get("co2_this_query_g", 0),
                "co2_saved_g":             receipt_raw.get("co2_saved_g", 0),
                "energy_saved_pct":        receipt_raw.get("energy_saved_pct", 0),
                "impact_reduction_pct":    receipt_raw.get("impact_reduction_pct", 0),
                "latency_ms":              receipt_raw.get("latency_ms", total_ms),
                "cache_hit":               receipt_raw.get("cache_hit", False),
                "similarity_score":        receipt_raw.get("similarity_score"),
            }),
        )

        # Print carbon summary if verbose mode on
        if self._client.verbose:
            _print_receipt(completion.carbon_receipt, model_used)

        return completion


class _Chat:
    """Mimics openai.resources.Chat"""
    def __init__(self, client: "GreenWeave"):
        self.completions = _Completions(client)


class GreenWeave:
    """
    Drop-in replacement for openai.OpenAI

    Usage:
        from greenweave_sdk import GreenWeave as OpenAI
        client = OpenAI(
            api_key="sk-...",                        # passed through, not used by GreenWeave
            greenweave_url="http://localhost:8000",  # your router URL
            team="engineering",                      # optional: for leaderboard tracking
            verbose=True,                            # optional: print carbon receipt
        )
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": "Hello!"}]
        )
        print(response.choices[0].message.content)
        print(f"COâ‚‚ saved: {response.carbon_receipt.co2_saved_g}g")
    """

    def __init__(
        self,
        api_key: str = None,
        greenweave_url: str = "http://localhost:8000",
        team: str = None,
        verbose: bool = False,
        timeout: int = 60,
        **kwargs,   # absorb base_url, organization, etc.
    ):
        self.api_key         = api_key
        self.greenweave_url  = greenweave_url.rstrip("/")
        self.team            = team
        self.verbose         = verbose
        self.timeout         = timeout
        self.chat            = _Chat(self)

    def get_carbon_status(self) -> dict:
        """Current grid carbon intensity."""
        try:
            return requests.get(f"{self.greenweave_url}/carbon/status", timeout=5).json()
        except Exception as e:
            return {"error": str(e)}

    def get_cache_stats(self) -> dict:
        """Semantic cache hit rate and COâ‚‚ saved by cache."""
        try:
            return requests.get(f"{self.greenweave_url}/cache/stats", timeout=5).json()
        except Exception as e:
            return {"error": str(e)}

    def get_esg_stats(self) -> dict:
        """Full ESG aggregate stats."""
        try:
            return requests.get(f"{self.greenweave_url}/stats", timeout=5).json()
        except Exception as e:
            return {"error": str(e)}

    def set_carbon_budget(self, limit_g: float) -> dict:
        """Set monthly COâ‚‚ budget in grams."""
        try:
            return requests.post(f"{self.greenweave_url}/budget/set",
                                 json={"limit_g": limit_g}, timeout=5).json()
        except Exception as e:
            return {"error": str(e)}

    def __repr__(self):
        return f"GreenWeave(url={self.greenweave_url}, team={self.team})"


# â”€â”€ Pretty print helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _print_receipt(receipt: "CarbonReceipt", model: str):
    is_cache = receipt.cache_hit
    if is_cache:
        print(f"\n  ðŸ§  CACHE HIT  |  COâ‚‚: 0.00000g  |  Energy saved: 100%  |  {receipt.latency_ms:.0f}ms\n")
    else:
        print(f"\n  âš¡ {receipt.mode}  |  Model: {model}  |  "
              f"COâ‚‚: {receipt.co2_this_query_g:.5f}g  |  "
              f"Saved: +{receipt.co2_saved_g:.4f}g  |  "
              f"Energy: {receipt.energy_saved_pct:.0f}%  |  "
              f"{receipt.latency_ms:.0f}ms\n")
