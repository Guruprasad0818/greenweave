# ðŸŒ¿ GreenWeave SDK

> **Drop-in OpenAI replacement. One line change. Your entire app becomes carbon-aware.**

## Install

```bash
pip install greenweave-sdk
# or from source:
pip install -e ./greenweave_sdk
```

## Usage

```python
# BEFORE â€” standard OpenAI
from openai import OpenAI
client = OpenAI(api_key="sk-...")

# AFTER â€” GreenWeave (change ONE line)
from greenweave_sdk import GreenWeave as OpenAI
client = OpenAI(api_key="sk-...", greenweave_url="http://localhost:8000")

# Everything below is IDENTICAL â€” zero breaking changes
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)  # âœ… works exactly like OpenAI

# Bonus: carbon data on every response
print(response.carbon_receipt.co2_saved_g)      # grams COâ‚‚ saved
print(response.carbon_receipt.energy_saved_pct) # % energy saved
print(response.carbon_receipt.cache_hit)         # True if served from cache
```

## What happens behind the scenes

```
Your app calls .create()
        â†“
GreenWeave SDK intercepts
        â†“
Check Semantic Cache (similarity > 0.92?)
  YES â†’ return instantly, 0 COâ‚‚, ~2ms âœ…
  NO  â†“
Check live grid carbon intensity
        â†“
Route to optimal model:
  LOW grid  (< 250 gCOâ‚‚/kWh) â†’ Llama 70B (full quality)
  MED grid  (250-500)         â†’ Llama 8B  (balanced)
  HIGH grid (> 500)           â†’ Llama 3B  (eco max)
        â†“
Return OpenAI-compatible response + carbon receipt
```

## Carbon Receipt

Every response includes a `carbon_receipt` object:

```python
response.carbon_receipt.mode                    # "ECO_LIGHT", "STANDARD", "SEMANTIC_CACHE"
response.carbon_receipt.model_used              # actual model that ran
response.carbon_receipt.grid_intensity_gco2_kwh # live grid carbon
response.carbon_receipt.co2_this_query_g        # COâ‚‚ emitted this query
response.carbon_receipt.co2_saved_g             # COâ‚‚ saved vs baseline
response.carbon_receipt.energy_saved_pct        # % energy saved
response.carbon_receipt.cache_hit               # True = 0 COâ‚‚, ~2ms
response.carbon_receipt.latency_ms              # total latency
```

## Advanced Options

```python
client = OpenAI(
    api_key="sk-...",
    greenweave_url="http://localhost:8000",
    team="engineering",        # team name for leaderboard
    verbose=True,              # print carbon receipt to terminal
    timeout=60,                # request timeout in seconds
)

# Force task-specific routing
response = client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    task_type="coding",          # casual_chat | coding | medical | legal_drafting
    weight_profile="ECO_FIRST",  # BALANCED | ECO_FIRST | ACCURACY_FIRST
    skip_cache=False,            # set True to bypass cache (testing)
)

# Direct utility methods
client.get_carbon_status()   # live grid intensity
client.get_cache_stats()     # cache hit rate, COâ‚‚ saved
client.get_esg_stats()       # full ESG aggregate data
client.set_carbon_budget(2000)  # set 2kg monthly COâ‚‚ budget
```

## Demo

```bash
python sdk_demo.py
```

Expected output:
```
ðŸ“¤ Query 1: First time asking â€” routes to optimal model
  âš¡ ECO_LIGHT | Model: llama-3.1-8b-instant | COâ‚‚: 0.00014g | Saved: +0.0027g | 80% | 842ms

ðŸ“¤ Query 2: Similar question â€” SEMANTIC CACHE INTERCEPTS IT
  ðŸ§  CACHE HIT | COâ‚‚: 0.00000g | Energy saved: 100% | 2ms
```

## Why This Matters

Any company using OpenAI today â€” **zero code changes required beyond one import**.
GreenWeave integrates into existing AI infrastructure in under 60 seconds.

---
*Built for AI4Dev Hackathon | Team PS100060 | GreenWeave*
